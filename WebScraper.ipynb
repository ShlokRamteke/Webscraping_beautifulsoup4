{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of WebScraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSPaJReIu+BYtVXY2DlOdL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShlokRamteke/Webscraping_beautifulsoup4/blob/main/WebScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFUif60-8vX7"
      },
      "source": [
        "## Scraping Medium sites data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wypfcDr--2Mn"
      },
      "source": [
        "# Installing the required libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl20L1Re-7dP"
      },
      "source": [
        "The contents fo a webpage can be downloaded using a library called request. It downloads the HTML data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp8NfkYm-xhL"
      },
      "source": [
        "# Install the library\n",
        "!pip install requests --upgrade --quiet\n",
        "!pip install pandas==1.1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGBs-Lqa_He9"
      },
      "source": [
        "# Import the library\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk0aJ49mNMI5"
      },
      "source": [
        "urls = {\n",
        "    'Towards Data Science': 'https://towardsdatascience.com/archive/{0}/{1:02d}/{2:02d}',\n",
        "    'Data Driven Investor': 'https://medium.com/datadriveninvestor/archive/{0}/{1:02d}/{2:02d}',\n",
        "    'Better Humans': 'https://medium.com/better-humans/archive/{0}/{1:02d}/{2:02d}',\n",
        "    'Better Marketing': 'https://medium.com/better-marketing/archive/{0}/{1:02d}/{2:02d}',\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWfOOJ_fNRGf"
      },
      "source": [
        "\n",
        "\n",
        "def is_leap(year):\n",
        "    if year % 4 != 0:\n",
        "        return False\n",
        "    elif year % 100 != 0:\n",
        "        return True\n",
        "    elif year % 400 != 0:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "    \n",
        "def convert_day(day, year):\n",
        "    month_days = [31, 29 if is_leap(year) else 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
        "    m = 0\n",
        "    d = 0\n",
        "    while day > 0:\n",
        "        d = day\n",
        "        day -= month_days[m]\n",
        "        m += 1\n",
        "    return (m, d)\n",
        "\n",
        "def get_claps(claps_str):\n",
        "    if (claps_str is None) or (claps_str == '') or (claps_str.split is None):\n",
        "        return 0\n",
        "    split = claps_str.split('K')\n",
        "    claps = float(split[0])\n",
        "    claps = int(claps*1000) if len(split) == 2 else int(claps)\n",
        "    return claps\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYdlIFUuOUC2"
      },
      "source": [
        "year = 2020\n",
        "selected_days = random.sample([i for i in range(1, 367 if is_leap(year) else 366)], 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4pjh4WdOxKu"
      },
      "source": [
        "img_dir = 'images'\n",
        "if not os.path.exists(img_dir):\n",
        "    os.mkdir(img_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqJGcqiTRxDc"
      },
      "source": [
        "data = []\n",
        "article_id = 0\n",
        "i = 0\n",
        "n = len(selected_days)\n",
        "for d in selected_days:\n",
        "    i += 1\n",
        "    month, day = convert_day(d, year)\n",
        "    date = '{0}-{1:02d}-{2:02d}'.format(year, month, day)\n",
        "    print(f'{i} / {n} ; {date}')\n",
        "    for publication, url in urls.items():\n",
        "        response = requests.get(url.format(year, month, day), allow_redirects=True)\n",
        "        if not response.url.startswith(url.format(year, month, day)):\n",
        "            continue\n",
        "        page = response.content\n",
        "        soup = BeautifulSoup(page, 'html.parser')\n",
        "        articles = soup.find_all(\"div\", class_=\"postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls\")\n",
        "        for article in articles:\n",
        "            title = article.find(\"h3\", class_=\"graf--title\")\n",
        "            if title is None:\n",
        "                continue\n",
        "            title = title.contents[0]\n",
        "            article_id += 1\n",
        "            subtitle = article.find(\"h4\", class_=\"graf--subtitle\")\n",
        "            subtitle = subtitle.contents[0] if subtitle is not None else ''\n",
        "            article_url = article.find_all(\"a\")[3]['href'].split('?')[0]\n",
        "            claps = get_claps(article.find_all(\"button\")[1].contents[0])\n",
        "            reading_time = article.find(\"span\", class_=\"readingTime\")\n",
        "            reading_time = 0 if reading_time is None else int(reading_time['title'].split(' ')[0])\n",
        "            responses = article.find_all(\"a\")\n",
        "            if len(responses) == 7:\n",
        "                responses = responses[6].contents[0].split(' ')\n",
        "                if len(responses) == 0:\n",
        "                    responses = 0\n",
        "                else:\n",
        "                    responses = responses[0]\n",
        "            else:\n",
        "                responses = 0\n",
        "\n",
        "            data.append([article_id, article_url, title, subtitle, claps, responses, reading_time, publication, date])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt54b8OXSRYk"
      },
      "source": [
        "medium_df = pd.DataFrame(data, columns=['id', 'url', 'title', 'subtitle', 'claps', 'responses', 'reading_time', 'publication', 'date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7aBCITJXSn_"
      },
      "source": [
        "medium_df.to_csv('medium_data.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V07rZCs9XiDn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}