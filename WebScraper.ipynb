{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of WebScraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMc/nuGX+Bb04uTqJp3W29r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShlokRamteke/Webscraping_beautifulsoup4/blob/main/WebScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFUif60-8vX7"
      },
      "source": [
        "## Scraping Top Repositories for Topics on GitHub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQOFbgJH84C9"
      },
      "source": [
        "Purpose of the notebook:\n",
        "> * Downloading web pages using the requests library\n",
        "> * Inspecting the HTML source code of a web page\n",
        "> * Parsing parts of a website using Beautiful Soup\n",
        "> * Writing parsed information into CSV files\n",
        "> * Using a REST API to retrieve data as JSON\n",
        "> * Combining data from multiple sources\n",
        "> * Using links on a page to crawl a website"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O7SPVQ295L6"
      },
      "source": [
        "Steps followed:\n",
        "> * We're going to scrape https://github.com/topics\n",
        ">* We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
        ">* For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
        ">* For each repository, we'll grab the repo name, username, stars and repo URL\n",
        ">* For each topic we'll create a CSV file in the following format:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wypfcDr--2Mn"
      },
      "source": [
        "# Downloading the web page using requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl20L1Re-7dP"
      },
      "source": [
        "The contents fo a webpage can be downloaded using a library called request. It downloads the HTML data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp8NfkYm-xhL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef5afcf-dfe4-4f66-9c0a-1af4f97ec115"
      },
      "source": [
        "# Install the library\n",
        "!pip install requests --upgrade --quiet\n",
        "!pip install pandas==1.1.5"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 20 kB 29.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 30 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 40 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 51 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 61 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 62 kB 690 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGBs-Lqa_He9"
      },
      "source": [
        "# Import the library\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk0aJ49mNMI5"
      },
      "source": [
        "urls = {\n",
        "    'Towards Data Science': 'https://towardsdatascience.com/archive/{0}/{1:02d}/{2:02d}',\n",
        "    'Data Driven Investor': 'https://medium.com/datadriveninvestor/archive/{0}/{1:02d}/{2:02d}',\n",
        "    'Better Humans': 'https://medium.com/better-humans/archive/{0}/{1:02d}/{2:02d}',\n",
        "    'Better Marketing': 'https://medium.com/better-marketing/archive/{0}/{1:02d}/{2:02d}',\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWfOOJ_fNRGf"
      },
      "source": [
        "\n",
        "\n",
        "def is_leap(year):\n",
        "    if year % 4 != 0:\n",
        "        return False\n",
        "    elif year % 100 != 0:\n",
        "        return True\n",
        "    elif year % 400 != 0:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "    \n",
        "def convert_day(day, year):\n",
        "    month_days = [31, 29 if is_leap(year) else 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
        "    m = 0\n",
        "    d = 0\n",
        "    while day > 0:\n",
        "        d = day\n",
        "        day -= month_days[m]\n",
        "        m += 1\n",
        "    return (m, d)\n",
        "\n",
        "def get_claps(claps_str):\n",
        "    if (claps_str is None) or (claps_str == '') or (claps_str.split is None):\n",
        "        return 0\n",
        "    split = claps_str.split('K')\n",
        "    claps = float(split[0])\n",
        "    claps = int(claps*1000) if len(split) == 2 else int(claps)\n",
        "    return claps\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYdlIFUuOUC2"
      },
      "source": [
        "year = 2020\n",
        "selected_days = random.sample([i for i in range(1, 367 if is_leap(year) else 366)], 50)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4pjh4WdOxKu"
      },
      "source": [
        "img_dir = 'images'\n",
        "if not os.path.exists(img_dir):\n",
        "    os.mkdir(img_dir)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqJGcqiTRxDc"
      },
      "source": [
        "data = []\n",
        "article_id = 0\n",
        "i = 0\n",
        "n = len(selected_days)\n",
        "for d in selected_days:\n",
        "    i += 1\n",
        "    month, day = convert_day(d, year)\n",
        "    date = '{0}-{1:02d}-{2:02d}'.format(year, month, day)\n",
        "    print(f'{i} / {n} ; {date}')\n",
        "    for publication, url in urls.items():\n",
        "        response = requests.get(url.format(year, month, day), allow_redirects=True)\n",
        "        if not response.url.startswith(url.format(year, month, day)):\n",
        "            continue\n",
        "        page = response.content\n",
        "        soup = BeautifulSoup(page, 'html.parser')\n",
        "        articles = soup.find_all(\"div\", class_=\"postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls\")\n",
        "        for article in articles:\n",
        "            title = article.find(\"h3\", class_=\"graf--title\")\n",
        "            if title is None:\n",
        "                continue\n",
        "            title = title.contents[0]\n",
        "            article_id += 1\n",
        "            subtitle = article.find(\"h4\", class_=\"graf--subtitle\")\n",
        "            subtitle = subtitle.contents[0] if subtitle is not None else ''\n",
        "            article_url = article.find_all(\"a\")[3]['href'].split('?')[0]\n",
        "            claps = get_claps(article.find_all(\"button\")[1].contents[0])\n",
        "            reading_time = article.find(\"span\", class_=\"readingTime\")\n",
        "            reading_time = 0 if reading_time is None else int(reading_time['title'].split(' ')[0])\n",
        "            responses = article.find_all(\"a\")\n",
        "            if len(responses) == 7:\n",
        "                responses = responses[6].contents[0].split(' ')\n",
        "                if len(responses) == 0:\n",
        "                    responses = 0\n",
        "                else:\n",
        "                    responses = responses[0]\n",
        "            else:\n",
        "                responses = 0\n",
        "\n",
        "            data.append([article_id, article_url, title, subtitle, claps, responses, reading_time, publication, date])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt54b8OXSRYk"
      },
      "source": [
        "medium_df = pd.DataFrame(data, columns=['id', 'url', 'title', 'subtitle', 'claps', 'responses', 'reading_time', 'publication', 'date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7aBCITJXSn_"
      },
      "source": [
        "medium_df.to_csv('medium_data.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V07rZCs9XiDn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}